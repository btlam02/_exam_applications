# assessment/services/question_pipeline.py
from typing import List, Dict, Any, Optional

from django.db import transaction

from ..models import (
    Subject,
    Topic,
    Question,
    QuestionOption,
    QuestionIRT,
    CandidateQuestion,
)


from ..services.llm_generation import generate_candidates_from_llm
from ..services.llm_evaluation import (
    build_deepseek_eval_prompt,
    call_deepseek_for_eval,
    compute_overall_score,
    should_auto_accept,
)


def _compute_question_difficulty_score(q: Question) -> float:
    """
    Chu·∫©n h√≥a ƒë·ªô kh√≥ c√¢u h·ªèi v·ªÅ kho·∫£ng [0,1] ƒë·ªÉ ƒë∆∞a cho LLM l√†m v√≠ d·ª•.

    ∆Øu ti√™n:
    1) N·∫øu c√≥ IRT.b (th∆∞·ªùng trong [-3, 3]) -> map tuy·∫øn t√≠nh sang [0,1]
    2) N·∫øu c√≥ difficulty_tag (easy/medium/hard) -> map s∆° b·ªô
    3) N·∫øu kh√¥ng c√≥ g√¨ -> 0.5
    """
    # 1) D·ª±a tr√™n IRT n·∫øu c√≥
    if hasattr(q, "irt") and q.irt is not None and q.irt.b is not None:
        b = float(q.irt.b)
        # map t·ª´ [-3, 3] -> [0, 1]
        b_clamped = max(-3.0, min(3.0, b))
        return (b_clamped + 3.0) / 6.0

    # 2) D·ª±a tr√™n difficulty_tag
    if q.difficulty_tag:
        tag_map = {
            "easy": 0.25,
            "medium": 0.5,
            "hard": 0.75,
        }
        return tag_map.get(q.difficulty_tag.lower(), 0.5)

    # 3) M·∫∑c ƒë·ªãnh
    return 0.5


def get_seed_questions(subject_id: int, topic_id: Optional[int], k: int = 5) -> List[Dict[str, Any]]:
    """
    L·∫•y k c√¢u h·ªèi m·∫´u (seed) t·ª´ DB cho c√πng m√¥n + (n·∫øu c√≥) c√πng topic.

    Tr·∫£ v·ªÅ list dict:
    [
      {
        "stem": "...",
        "options": [{label, content, is_correct}],
        "difficulty_score": float in [0,1],
      },
      ...
    ]
    """
    qs = Question.objects.filter(subject_id=subject_id)

    # üîß QUAN TR·ªåNG: d√πng related_name="tags" tr√™n QuestionTag
    # KH√îNG d√πng 'tag__topic_id' hay 'questiontag__topic_id'
    if topic_id is not None:
        qs = qs.filter(tags__topic_id=topic_id)

    # Prefetch ƒë·ªÉ tr√°nh N+1 query
    qs = (
        qs.prefetch_related("options", "irt", "stats", "tags")
          .order_by("?")[:k]
    )

    items: List[Dict[str, Any]] = []
    for q in qs:
        opts = [
            {
                "label": o.label,
                "content": o.content,
                "is_correct": o.is_correct,
            }
            for o in q.options.all().order_by("label")
        ]

        diff_score = _compute_question_difficulty_score(q)

        items.append(
            {
                "stem": q.stem,
                "options": opts,
                "difficulty_score": diff_score,
            }
        )

    return items


@transaction.atomic
def generate_candidate_questions(
    subject: Subject,
    topic: Topic,
    target_difficulty: str,
    num_questions: int,
) -> List[CandidateQuestion]:
    """
    Pipeline ƒë·∫ßy ƒë·ªß:

    1) L·∫•y seed questions (c√πng subject + topic n·∫øu c√≥)
    2) G·ªçi Gemini sinh c√¢u h·ªèi m·ªõi
    3) M·ªói c√¢u m·ªõi:
        - G·ª≠i sang DeepSeek ƒë·ªÉ ƒë√°nh gi√° ƒë·ªãnh t√≠nh/ƒë·ªãnh l∆∞·ª£ng
        - T√≠nh c√°c metric (difficulty_alignment, agreement, overall_score)
        - L∆∞u v√†o CandidateQuestion (status: accepted/pending)
    """
    # 1) Seed t·ª´ DB
    seed_items = get_seed_questions(subject.id, topic.id, k=5)

    # 2) G·ªçi Gemini sinh c√¢u h·ªèi m·ªõi
    candidates_raw = generate_candidates_from_llm(
        seed_items=seed_items,
        subject_name=subject.name,
        topic_name=topic.name,
        target_difficulty=target_difficulty,
        num_questions=num_questions,
    )

    result: List[CandidateQuestion] = []

    for cand_raw in candidates_raw:
        stem = cand_raw.get("question")
        options = cand_raw.get("options", [])
        answer = cand_raw.get("answer", "A")

        if not stem or len(options) < 2:
            # B·ªè qua c√¢u l·ªói/thi·∫øu data
            continue

        answer = str(answer).strip().upper()

        diff_g = cand_raw.get("difficulty_score")
        diff_label_g = cand_raw.get("difficulty_label")

        # 3) ƒê√°nh gi√° b·∫±ng DeepSeek (ƒë·ªãnh t√≠nh + ƒë·ªãnh l∆∞·ª£ng)
        eval_prompt = build_deepseek_eval_prompt(seed_items, cand_raw)
        eval_metrics = call_deepseek_for_eval(eval_prompt)

        # B·ªï sung c√°c ch·ªâ s·ªë ƒë·ªãnh l∆∞·ª£ng: difficulty_alignment, agreement, overall_score
        eval_metrics = compute_overall_score(
            eval_metrics,
            target_difficulty=target_difficulty,
            d_gemini=diff_g,
        )

        auto_accept = should_auto_accept(eval_metrics)

        cq = CandidateQuestion.objects.create(
            subject=subject,
            topic=topic,
            stem=stem,
            options_json=options,  # list string ["...", "..."]
            correct_answer=answer,
            target_difficulty=target_difficulty,
            difficulty_score_gemini=diff_g,
            difficulty_label_gemini=diff_label_g,
            difficulty_score_deepseek=eval_metrics.get("difficulty_score_deepseek"),
            difficulty_label_deepseek=eval_metrics.get("difficulty_label_deepseek"),
            validity=eval_metrics.get("validity"),
            on_topic=eval_metrics.get("on_topic"),
            clarity=eval_metrics.get("clarity"),
            single_correct=eval_metrics.get("single_correct"),
            similarity_to_examples=eval_metrics.get("similarity_to_examples"),
            overall_score=eval_metrics.get("overall_score"),
            comment=eval_metrics.get("comment"),
            status="accepted" if auto_accept else "pending",
        )
        result.append(cq)

        # N·∫øu mu·ªën auto-promote ngay khi auto_accept, c√≥ th·ªÉ g·ªçi promote_candidate_to_question(cq)
        # ·ªü ƒë√¢y (ho·∫∑c ƒë·ªÉ admin duy·ªát th·ªß c√¥ng).

    return result


@transaction.atomic
def promote_candidate_to_question(candidate: CandidateQuestion) -> Question:
    """
    Chuy·ªÉn m·ªôt CandidateQuestion (ƒë√£ ƒë∆∞·ª£c duy·ªát) th√†nh Question + QuestionOption + QuestionIRT.

    - Question: stem, subject, item_type="MCQ", difficulty_tag: l·∫•y t·ª´ DeepSeek/Gemini
    - Options: t·ª´ options_json, g√°n A/B/C/D..., x√°c ƒë·ªãnh ƒë√°p √°n ƒë√∫ng theo correct_answer
    - IRT: t·∫°o b·∫£n ghi QuestionIRT v·ªõi (a, b, c) suy t·ª´ difficulty_score c·ªßa 2 LLM
    """
    # 1) T·∫°o Question
    difficulty_tag = (
        candidate.difficulty_label_deepseek
        or candidate.difficulty_label_gemini
        or candidate.target_difficulty
    )

    q = Question.objects.create(
        subject=candidate.subject,
        stem=candidate.stem,
        item_type="MCQ",
        difficulty_tag=difficulty_tag,
        # KH√îNG c√≥ field difficulty_score tr√™n Question, n√™n kh√¥ng truy·ªÅn v√†o
    )

    # 2) T·∫°o QuestionOption
    options = candidate.options_json or []
    # correct_answer: "A" -> index 0, "B" -> 1, ...
    try:
        correct_idx = ord(candidate.correct_answer.strip().upper()) - ord("A")
    except Exception:
        correct_idx = 0

    for i, content in enumerate(options):
        QuestionOption.objects.create(
            question=q,
            label=chr(ord("A") + i),
            content=str(content),
            is_correct=(i == correct_idx),
        )

    # 3) T·∫°o IRT: s·ª≠ d·ª•ng trung b√¨nh difficulty_score t·ª´ Gemini + DeepSeek
    d_g = candidate.difficulty_score_gemini
    d_d = candidate.difficulty_score_deepseek

    if d_g is None and d_d is None:
        b_score = 0.5
    elif d_g is None:
        b_score = float(d_d)
    elif d_d is None:
        b_score = float(d_g)
    else:
        b_score = (float(d_g) + float(d_d)) / 2.0

    # map [0,1] -> [-1,1] t·∫°m (c√≥ th·ªÉ thay b·∫±ng [-3,3] n·∫øu b·∫°n mu·ªën s√°t IRT h∆°n)
    b = 2.0 * (b_score - 0.5)
    a = 1.0    # discrimination m·∫∑c ƒë·ªãnh
    c = 0.25   # guessing m·∫∑c ƒë·ªãnh

    QuestionIRT.objects.create(
        question=q,
        a=a,
        b=b,
        c=c,
    )

    # 4) C·∫≠p nh·∫≠t tr·∫°ng th√°i CandidateQuestion
    candidate.status = "accepted"
    candidate.save(update_fields=["status"])

    return q
